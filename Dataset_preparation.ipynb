{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LzB6Z6yneure",
    "outputId": "2e376559-f1af-43e5-b0c4-b718343ca4ce"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\r\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\r\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\r\n",
      "Installing collected packages: tqdm\r\n",
      "Successfully installed tqdm-4.66.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T09:00:22.005149Z",
     "start_time": "2024-01-30T09:00:20.334883Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import pprint\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import tensorflow as tf\n",
    "from numpy import expand_dims\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from itertools import product"
   ],
   "metadata": {
    "id": "yaGSEAAxe__E",
    "ExecuteTime": {
     "end_time": "2024-02-19T16:38:57.759408Z",
     "start_time": "2024-02-19T16:38:53.753005Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def count_images(folder_path):\n",
    "    total_files = 0\n",
    "    if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "        for foldername, subfolders, filenames in os.walk(folder_path):\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(foldername, filename)\n",
    "                if os.path.isfile(file_path) and (file_path.endswith('.jpg') or file_path.endswith('.jpeg') or file_path.endswith('.JPG') or file_path.endswith('.JPEG')):\n",
    "                    total_files += 1\n",
    "    return total_files\n",
    "\n",
    "def partition_file_list(ratios, items):\n",
    "    \"\"\"\n",
    "    :param ratios: [0.15, 0.15, 0.7]\n",
    "    :param items: [\"f0\", \"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"f6\", \"f7\", \"f8\", \"f9\"]\n",
    "    :return: [[\"f0\"],  #truncates amount to the lowest integer\n",
    "              [\"f1\"],  #truncates to the lowest integer\n",
    "              [\"f2\", \"f3\", \"f4\", \"f5\", \"f6\", \"f7\", \"f8\", \"f9\"] #truncates to the lowest integer\n",
    "              ]\n",
    "\n",
    "    \"\"\"\n",
    "    total_items = len(items)\n",
    "    partitions = []\n",
    "    start = 0\n",
    "    for i, ratio in enumerate(ratios):\n",
    "        end = start + int(math.ceil(ratio * total_items))\n",
    "        if i == len(ratios)-1:\n",
    "            partitions.append(items[start:])\n",
    "        else:\n",
    "            partitions.append(items[start:end])\n",
    "            start = end\n",
    "    return partitions\n",
    "\n",
    "def create_splitted_folders(src_dataset, dst_dataset):\n",
    "    \"\"\"\n",
    "    :param src_dataset: {\n",
    "        'esca':    os.path.join(base_folder, 'esca_dataset_unsplitted', 'esca'),\n",
    "        'healthy': os.path.join(base_folder, 'esca_dataset_unsplitted', 'healthy')\n",
    "    }\n",
    "    :param dst_dataset: {\n",
    "        'train': {\n",
    "            'esca':    os.path.join(base_folder, 'esca_dataset', 'train', 'esca'),\n",
    "            'healthy': os.path.join(base_folder, 'esca_dataset', 'train', 'healthy'),\n",
    "            'ratio': 0.7\n",
    "        },\n",
    "        'test': {\n",
    "            'esca':    os.path.join(base_folder, 'esca_dataset', 'test', 'esca'),\n",
    "            'healthy': os.path.join(base_folder, 'esca_dataset', 'test', 'healthy'),\n",
    "            'ratio': 0.15\n",
    "        },\n",
    "        'validation': {\n",
    "            'esca':    os.path.join(base_folder, 'esca_dataset', 'validation', 'esca'),\n",
    "            'healthy': os.path.join(base_folder, 'esca_dataset', 'validation', 'healthy'),\n",
    "            'ratio': 0.15\n",
    "        }\n",
    "    }\n",
    "    :return: creates the folders indicated in the record dst_dataset copying the files from src_dataset\n",
    "    \"\"\"\n",
    "    for class_name in ['esca', 'healthy']:\n",
    "        src_file_list = os.listdir(src_dataset[class_name])\n",
    "        random.seed(1)# set seed fo replicability\n",
    "        random.shuffle(src_file_list)\n",
    "        test_file_list, validation_file_list, train_file_list = partition_file_list(\n",
    "            [dst_dataset['test']['ratio'], dst_dataset['validation']['ratio'], dst_dataset['train']['ratio']],\n",
    "            src_file_list\n",
    "        )\n",
    "        partition_info = [\n",
    "            {'name': 'test', 'file_list': test_file_list},\n",
    "            {'name': 'validation', 'file_list': validation_file_list},\n",
    "            {'name': 'train', 'file_list': train_file_list}\n",
    "        ]\n",
    "        for record in partition_info:\n",
    "            split_name = record['name']\n",
    "            if not (os.path.exists(dst_dataset[split_name][class_name]) and os.path.isdir(dst_dataset[split_name][class_name])):\n",
    "                os.makedirs(dst_dataset[split_name][class_name], exist_ok=True)\n",
    "            print(f\"Creating folder {split_name}/{class_name}\")\n",
    "            for file_name in tqdm(record['file_list']):\n",
    "                try:\n",
    "                    shutil.copy(\n",
    "                        os.path.join(src_dataset[class_name], file_name),\n",
    "                        os.path.join(dst_dataset[split_name][class_name], file_name)\n",
    "                    )\n",
    "                    #print(\"File copied successfully.\")\n",
    "                except:\n",
    "                    print(f\"Error occurred while copying file {file_name}\")\n",
    "\n",
    "    print(\"Data partitioning completed.\")\n",
    "\n",
    "def find_duplicate_strings(L):\n",
    "    occurrences = {}\n",
    "    duplicates = []\n",
    "    for string in L:\n",
    "        # Count occurrences of each string using a dictionary\n",
    "        occurrences[string] = occurrences.get(string, 0) + 1\n",
    "\n",
    "    # Find strings that occur more than once\n",
    "    for string, count in occurrences.items():\n",
    "        if count > 1:\n",
    "            duplicates.append(string)\n",
    "    return duplicates"
   ],
   "metadata": {
    "id": "YpADMxfXZncg",
    "ExecuteTime": {
     "end_time": "2024-02-19T16:38:59.773166Z",
     "start_time": "2024-02-19T16:38:59.741313Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "base_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "os.chdir(base_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T16:39:02.009613Z",
     "start_time": "2024-02-19T16:39:01.972727Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**THE PICTURES ARE ALL STORED INTO TWO FOLDERS:**\n",
    "\n",
    "* f\"{base_folder}/esca_dataset_unsplitted/esca\"\n",
    "* f\"{base_folder}/esca_dataset_unsplitted/healthy\""
   ],
   "metadata": {
    "id": "AyTQ88WMfR6y",
    "ExecuteTime": {
     "end_time": "2024-02-19T11:00:32.413752Z",
     "start_time": "2024-02-19T11:00:32.383344Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "base_folder = '.'\n",
    "data_folders = {\n",
    "    '.': base_folder,\n",
    "    'esca_dataset_unsplitted': {\n",
    "        '.': os.path.join(base_folder, 'esca_dataset_unsplitted'),\n",
    "        'esca':    os.path.join(base_folder, 'esca_dataset_unsplitted', 'esca'),\n",
    "        'healthy': os.path.join(base_folder, 'esca_dataset_unsplitted', 'healthy')\n",
    "    },\n",
    "    'esca_dataset': {\n",
    "        '.': os.path.join(base_folder, 'esca_dataset'),\n",
    "        'train': {\n",
    "            '.': os.path.join(base_folder, 'esca_dataset', 'train'),\n",
    "            'esca':    os.path.join(base_folder, 'esca_dataset', 'train', 'esca'),\n",
    "            'healthy': os.path.join(base_folder, 'esca_dataset', 'train', 'healthy'),\n",
    "            'ratio': 0.7\n",
    "        },\n",
    "        'test': {\n",
    "            '.': os.path.join(base_folder, 'esca_dataset', 'test'),\n",
    "            'esca':    os.path.join(base_folder, 'esca_dataset', 'test', 'esca'),\n",
    "            'healthy': os.path.join(base_folder, 'esca_dataset', 'test', 'healthy'),\n",
    "            'ratio': 0.15\n",
    "        },\n",
    "        'validation': {\n",
    "            '.': os.path.join(base_folder, 'esca_dataset', 'validation'),\n",
    "            'esca':    os.path.join(base_folder, 'esca_dataset', 'validation', 'esca'),\n",
    "            'healthy': os.path.join(base_folder, 'esca_dataset', 'validation', 'healthy'),\n",
    "            'ratio': 0.15\n",
    "        }\n",
    "    },\n",
    "    'esca_dataset_swapped_bg_unsplitted': {\n",
    "        '.': os.path.join(base_folder, 'esca_dataset_swapped_bg_unsplitted'),\n",
    "        'esca':    os.path.join(base_folder, 'esca_dataset_swapped_bg_unsplitted', 'esca'),\n",
    "        'healthy': os.path.join(base_folder, 'esca_dataset_swapped_bg_unsplitted', 'healthy')\n",
    "    },\n",
    "    'esca_dataset_swapped_bg': {\n",
    "        '.': os.path.join(base_folder, 'esca_dataset_swapped_bg'),\n",
    "        'train': {\n",
    "            '.': os.path.join(base_folder, 'esca_dataset_swapped_bg', 'train'),\n",
    "            'esca':    os.path.join(base_folder, 'esca_dataset_swapped_bg', 'train', 'esca'),\n",
    "            'healthy': os.path.join(base_folder, 'esca_dataset_swapped_bg', 'train', 'healthy'),\n",
    "            'ratio': 0.7\n",
    "        },\n",
    "        'test': {\n",
    "            '.': os.path.join(base_folder, 'esca_dataset_swapped_bg', 'test'),\n",
    "            'esca':    os.path.join(base_folder, 'esca_dataset_swapped_bg', 'test', 'esca'),\n",
    "            'healthy': os.path.join(base_folder, 'esca_dataset_swapped_bg', 'test', 'healthy'),\n",
    "            'ratio': 0.15\n",
    "        },\n",
    "        'validation': {\n",
    "            '.': os.path.join(base_folder, 'esca_dataset_swapped_bg', 'validation'),\n",
    "            'esca':    os.path.join(base_folder, 'esca_dataset_swapped_bg', 'validation', 'esca'),\n",
    "            'healthy': os.path.join(base_folder, 'esca_dataset_swapped_bg', 'validation', 'healthy'),\n",
    "            'ratio': 0.15\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Serializing json\n",
    "json_object = json.dumps(data_folders, indent = 4)\n",
    "print(json_object)"
   ],
   "metadata": {
    "id": "nFV8xbd0fHqj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d07282ea-9e6e-432f-9f94-da7a28869b5c",
    "ExecuteTime": {
     "end_time": "2024-02-19T16:39:04.209282Z",
     "start_time": "2024-02-19T16:39:04.173293Z"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \".\": \".\",\n",
      "    \"esca_dataset_unsplitted\": {\n",
      "        \".\": \"./esca_dataset_unsplitted\",\n",
      "        \"esca\": \"./esca_dataset_unsplitted/esca\",\n",
      "        \"healthy\": \"./esca_dataset_unsplitted/healthy\"\n",
      "    },\n",
      "    \"esca_dataset\": {\n",
      "        \".\": \"./esca_dataset\",\n",
      "        \"train\": {\n",
      "            \".\": \"./esca_dataset/train\",\n",
      "            \"esca\": \"./esca_dataset/train/esca\",\n",
      "            \"healthy\": \"./esca_dataset/train/healthy\",\n",
      "            \"ratio\": 0.7\n",
      "        },\n",
      "        \"test\": {\n",
      "            \".\": \"./esca_dataset/test\",\n",
      "            \"esca\": \"./esca_dataset/test/esca\",\n",
      "            \"healthy\": \"./esca_dataset/test/healthy\",\n",
      "            \"ratio\": 0.15\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \".\": \"./esca_dataset/validation\",\n",
      "            \"esca\": \"./esca_dataset/validation/esca\",\n",
      "            \"healthy\": \"./esca_dataset/validation/healthy\",\n",
      "            \"ratio\": 0.15\n",
      "        }\n",
      "    },\n",
      "    \"esca_dataset_swapped_bg_unsplitted\": {\n",
      "        \".\": \"./esca_dataset_swapped_bg_unsplitted\",\n",
      "        \"esca\": \"./esca_dataset_swapped_bg_unsplitted/esca\",\n",
      "        \"healthy\": \"./esca_dataset_swapped_bg_unsplitted/healthy\"\n",
      "    },\n",
      "    \"esca_dataset_swapped_bg\": {\n",
      "        \".\": \"./esca_dataset_swapped_bg\",\n",
      "        \"train\": {\n",
      "            \".\": \"./esca_dataset_swapped_bg/train\",\n",
      "            \"esca\": \"./esca_dataset_swapped_bg/train/esca\",\n",
      "            \"healthy\": \"./esca_dataset_swapped_bg/train/healthy\",\n",
      "            \"ratio\": 0.7\n",
      "        },\n",
      "        \"test\": {\n",
      "            \".\": \"./esca_dataset_swapped_bg/test\",\n",
      "            \"esca\": \"./esca_dataset_swapped_bg/test/esca\",\n",
      "            \"healthy\": \"./esca_dataset_swapped_bg/test/healthy\",\n",
      "            \"ratio\": 0.15\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \".\": \"./esca_dataset_swapped_bg/validation\",\n",
      "            \"esca\": \"./esca_dataset_swapped_bg/validation/esca\",\n",
      "            \"healthy\": \"./esca_dataset_swapped_bg/validation/healthy\",\n",
      "            \"ratio\": 0.15\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**DATA AUGMENTATION IN TRAIN AND VALIDATION FOLDERS**"
   ],
   "metadata": {
    "id": "w1IfhMrl5Tb0",
    "ExecuteTime": {
     "end_time": "2024-02-19T11:05:13.960650Z",
     "start_time": "2024-02-19T11:05:13.870094Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def blur(img):\n",
    "    return (cv2.blur(img,(30,30)))\n",
    "\n",
    "def horizontal_flip(img):\n",
    "    return (tf.image.flip_left_right(img))\n",
    "\n",
    "def vertical_flip(img):\n",
    "    return (tf.image.flip_up_down(img))\n",
    "\n",
    "def contrast(img):\n",
    "    return (tf.image.adjust_contrast(img, 0.5))\n",
    "\n",
    "def saturation(img):\n",
    "    return (tf.image.adjust_saturation(img, 3))\n",
    "\n",
    "def hue(img):\n",
    "    return (tf.image.adjust_hue(img, 0.1))\n",
    "\n",
    "def gamma(img):\n",
    "    return (tf.image.adjust_gamma(img, 2))\n",
    "\n",
    "def augment_with_13transformations(splitted_dataset):\n",
    "    \"\"\"\n",
    "    :param splitted_dataset:  {\n",
    "            'train': {\n",
    "                'esca':  ...,\n",
    "                'healthy':\n",
    "            }\n",
    "    }\n",
    "    :return: ...\n",
    "    \"\"\"\n",
    "\n",
    "    transformations = [\n",
    "        {'name' : 'horizontalFlip',\n",
    "        'datagen' : ImageDataGenerator(preprocessing_function=horizontal_flip)},\n",
    "        {'name' : 'verticalFlip',\n",
    "        'datagen' : ImageDataGenerator(preprocessing_function=vertical_flip)},\n",
    "        {'name' : 'rotation',\n",
    "        'datagen' : ImageDataGenerator(rotation_range = 40, fill_mode='nearest')},\n",
    "        {'name' : 'widthShift',\n",
    "        'datagen' : ImageDataGenerator(width_shift_range = 0.2, fill_mode='nearest')},\n",
    "        {'name' : 'heightShift',\n",
    "        'datagen' : ImageDataGenerator(height_shift_range = 0.2, fill_mode='nearest')},\n",
    "        {'name' : 'shearRange',\n",
    "        'datagen' : ImageDataGenerator(shear_range = 0.2)},\n",
    "        {'name' : 'zoom',\n",
    "        'datagen' : ImageDataGenerator(zoom_range = [0.5, 1.0])},\n",
    "        {'name' : 'blur',\n",
    "        'datagen' : ImageDataGenerator(preprocessing_function=blur)},\n",
    "        {'name' : 'brightness',\n",
    "        'datagen' : ImageDataGenerator(brightness_range = [1.1, 1.5])},\n",
    "        {'name' : 'contrast',\n",
    "        'datagen' : ImageDataGenerator(preprocessing_function=contrast)},\n",
    "        {'name' : 'saturation',\n",
    "        'datagen' : ImageDataGenerator(preprocessing_function=saturation)},\n",
    "        {'name' : 'hue',\n",
    "        'datagen' : ImageDataGenerator(preprocessing_function=hue)},\n",
    "        {'name' : 'gamma',\n",
    "        'datagen' : ImageDataGenerator(preprocessing_function=gamma)}\n",
    "    ]\n",
    "    split_class_list = list(product(['train'], ['esca', 'healthy']))\n",
    "    for s, c in split_class_list:\n",
    "        print(f\"Augmenting data in folder {splitted_dataset[s][c]}\")\n",
    "        list_of_files = [f for f in os.listdir(splitted_dataset[s][c]) if not f.startswith('.') and f.endswith('jpg')]\n",
    "        for filename in tqdm(list_of_files):\n",
    "            for t in transformations:\n",
    "                print(f\"Applying {t['name']} transformation to file {os.path.join(splitted_dataset[s][c], filename)}\")\n",
    "                img = load_img(os.path.join(splitted_dataset[s][c], filename))\n",
    "                data = img_to_array(img)\n",
    "                samples = expand_dims(data, 0)\n",
    "                it = t['datagen'].flow(samples, batch_size = 1,\n",
    "                    save_to_dir = splitted_dataset[s][c],\n",
    "                    save_prefix = '_'.join([Path(filename).stem, t['name']]),\n",
    "                    save_format ='jpg')\n",
    "                batch = it.next()\n"
   ],
   "metadata": {
    "id": "Al5GbQCL5QsV",
    "ExecuteTime": {
     "end_time": "2024-02-19T11:17:28.774760Z",
     "start_time": "2024-02-19T11:17:28.765931Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "create_splitted_folders(\n",
    "    data_folders['esca_dataset_unsplitted'],\n",
    "    data_folders['esca_dataset']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "create_splitted_folders(\n",
    "    data_folders['esca_dataset_swapped_bg_unsplitted'],\n",
    "    data_folders['esca_dataset_swapped_bg']\n",
    ")"
   ],
   "metadata": {
    "id": "Qey9SrcbWfw1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5d2ee556-1885-4701-b01a-ff43e91f7299"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esca_dataset_unsplitted: 1770 images.\n",
      "esca_dataset_swapped_bg_unsplitted: 1770 images.\n",
      "esca_dataset: 17851 images.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mesca_dataset_swapped_bg_unsplitted: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcount_images(data_folders[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mesca_dataset_swapped_bg_unsplitted\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m images.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mesca_dataset: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcount_images(data_folders[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mesca_dataset\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m images.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mesca_dataset_swapped_bg: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43mcount_images\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_folders\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mesca_dataset_swapped_bg\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m images.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mesca_dataset_mixed: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcount_images(data_folders[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mesca_dataset_mixed\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m images.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[3], line 7\u001B[0m, in \u001B[0;36mcount_images\u001B[0;34m(folder_path)\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m filenames:\n\u001B[1;32m      6\u001B[0m             file_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(foldername, filename)\n\u001B[0;32m----> 7\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43misfile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mand\u001B[39;00m (file_path\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.jpg\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m file_path\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.jpeg\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m file_path\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.JPG\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m file_path\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.JPEG\u001B[39m\u001B[38;5;124m'\u001B[39m)):\n\u001B[1;32m      8\u001B[0m                 total_files \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m total_files\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/genericpath.py:30\u001B[0m, in \u001B[0;36misfile\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Test whether a path is a regular file\"\"\"\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     st \u001B[38;5;241m=\u001B[39m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mOSError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"esca_dataset_unsplitted: {count_images(data_folders['esca_dataset_unsplitted']['.'])} images.\")\n",
    "print(f\"esca_dataset_swapped_bg_unsplitted: {count_images(data_folders['esca_dataset_swapped_bg_unsplitted']['.'])} images.\")\n",
    "print(f\"esca_dataset: {count_images(data_folders['esca_dataset']['.'])} images.\")\n",
    "print(f\"esca_dataset_swapped_bg: {count_images(data_folders['esca_dataset_swapped_bg']['.'])} images.\")\n",
    "print(f\"esca_dataset_mixed: {count_images(data_folders['esca_dataset_mixed']['.'])} images.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T18:17:49.172118Z",
     "start_time": "2024-02-19T18:16:59.118784Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data_folders_info = {\n",
    "    'esca_dataset_unsplitted': {\n",
    "        'esca':    count_images(data_folders['esca_dataset_unsplitted']['esca']),\n",
    "        'healthy': count_images(data_folders['esca_dataset_unsplitted']['healthy'])\n",
    "    },\n",
    "    'esca_dataset': {\n",
    "        'train': {\n",
    "            'esca':    count_images(data_folders['esca_dataset']['train']['esca']),\n",
    "            'healthy': count_images(data_folders['esca_dataset']['train']['healthy']),\n",
    "            'proportion': count_images(data_folders['esca_dataset']['train']['.'])/count_images(data_folders['esca_dataset']['.']) if count_images(data_folders['esca_dataset']['.']) > 0 else 0\n",
    "        },\n",
    "        'test': {\n",
    "            'esca':    count_images(data_folders['esca_dataset']['test']['esca']),\n",
    "            'healthy': count_images(data_folders['esca_dataset']['test']['healthy']),\n",
    "            'proportion': count_images(data_folders['esca_dataset']['test']['.'])/count_images(data_folders['esca_dataset']['.']) if count_images(data_folders['esca_dataset']['.']) > 0 else 0\n",
    "        },\n",
    "        'validation': {\n",
    "            'esca':    count_images(data_folders['esca_dataset']['validation']['esca']),\n",
    "            'healthy': count_images(data_folders['esca_dataset']['validation']['healthy']),\n",
    "            'proportion': count_images(data_folders['esca_dataset']['validation']['.'])/count_images(data_folders['esca_dataset']['.']) if count_images(data_folders['esca_dataset']['.']) > 0 else 0\n",
    "        }\n",
    "    },\n",
    "    'esca_dataset_swapped_bg_unsplitted': {\n",
    "        'esca':    count_images(data_folders['esca_dataset_swapped_bg_unsplitted']['esca']),\n",
    "        'healthy': count_images(data_folders['esca_dataset_swapped_bg_unsplitted']['healthy'])\n",
    "    },\n",
    "    'esca_dataset_swapped_bg': {\n",
    "        'train': {\n",
    "            'esca':    count_images(data_folders['esca_dataset_swapped_bg']['train']['esca']),\n",
    "            'healthy': count_images(data_folders['esca_dataset_swapped_bg']['train']['healthy']),\n",
    "            'proportion': count_images(data_folders['esca_dataset_swapped_bg']['train']['.'])/count_images(data_folders['esca_dataset_swapped_bg']['.']) if count_images(data_folders['esca_dataset_swapped_bg']['.']) > 0 else 0\n",
    "        },\n",
    "        'test': {\n",
    "            'esca':    count_images(data_folders['esca_dataset_swapped_bg']['test']['esca']),\n",
    "            'healthy': count_images(data_folders['esca_dataset_swapped_bg']['test']['healthy']),\n",
    "            'proportion': count_images(data_folders['esca_dataset_swapped_bg']['test']['.'])/count_images(data_folders['esca_dataset_swapped_bg']['.'])  if count_images(data_folders['esca_dataset_swapped_bg']['.']) > 0 else 0\n",
    "        },\n",
    "        'validation': {\n",
    "            'esca':    count_images(data_folders['esca_dataset_swapped_bg']['validation']['esca']),\n",
    "            'healthy': count_images(data_folders['esca_dataset_swapped_bg']['validation']['healthy']),\n",
    "            'proportion': count_images(data_folders['esca_dataset_swapped_bg']['validation']['.'])/count_images(data_folders['esca_dataset_swapped_bg']['.'])  if count_images(data_folders['esca_dataset_swapped_bg']['.']) > 0 else 0\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Serializing json\n",
    "json_object = json.dumps(data_folders_info, indent = 4)\n",
    "print(json_object)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c77Jbbl7JOHA",
    "outputId": "b6f8cf41-38ce-48fe-bbfc-73f8cd80c7d4",
    "ExecuteTime": {
     "end_time": "2024-02-19T12:31:46.011837Z",
     "start_time": "2024-02-19T12:31:43.304258Z"
    }
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"esca_dataset_unsplitted\": {\n",
      "        \"esca\": 888,\n",
      "        \"healthy\": 882\n",
      "    },\n",
      "    \"esca_dataset\": {\n",
      "        \"train\": {\n",
      "            \"esca\": 8680,\n",
      "            \"healthy\": 8638,\n",
      "            \"proportion\": 0.9701417287546916\n",
      "        },\n",
      "        \"test\": {\n",
      "            \"esca\": 134,\n",
      "            \"healthy\": 133,\n",
      "            \"proportion\": 0.014957145257968742\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"esca\": 134,\n",
      "            \"healthy\": 132,\n",
      "            \"proportion\": 0.014901125987339645\n",
      "        }\n",
      "    },\n",
      "    \"esca_dataset_swapped_bg_unsplitted\": {\n",
      "        \"esca\": 882,\n",
      "        \"healthy\": 888\n",
      "    },\n",
      "    \"esca_dataset_swapped_bg\": {\n",
      "        \"train\": {\n",
      "            \"esca\": 8638,\n",
      "            \"healthy\": 8694,\n",
      "            \"proportion\": 0.9702194357366771\n",
      "        },\n",
      "        \"test\": {\n",
      "            \"esca\": 133,\n",
      "            \"healthy\": 134,\n",
      "            \"proportion\": 0.014946260635915809\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"esca\": 132,\n",
      "            \"healthy\": 133,\n",
      "            \"proportion\": 0.014834303627407076\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "augment_with_13transformations(\n",
    "    splitted_dataset=data_folders['esca_dataset']\n",
    "    )"
   ],
   "metadata": {
    "id": "8rg4wkv5_PhO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "augment_with_13transformations(\n",
    "    splitted_dataset=data_folders['esca_dataset_swapped_bg']\n",
    "    )"
   ],
   "metadata": {
    "id": "GAkZVqY7bL-R",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5975fe74-33c1-4a29-89c5-92e1952b9e81"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing: ./esca_dataset_swapped_bg/.DS_Store\n",
      "Removed: ./esca_dataset_swapped_bg/.DS_Store\n",
      "Error processing: ./esca_dataset_swapped_bg/validation/healthy/.DS_Store\n",
      "Removed: ./esca_dataset_swapped_bg/validation/healthy/.DS_Store\n",
      "Error processing: ./esca_dataset_swapped_bg/validation/esca/.DS_Store\n",
      "Removed: ./esca_dataset_swapped_bg/validation/esca/.DS_Store\n",
      "Error processing: ./esca_dataset/.DS_Store\n",
      "Removed: ./esca_dataset/.DS_Store\n",
      "Error processing: ./esca_dataset/train/esca/.DS_Store\n",
      "Removed: ./esca_dataset/train/esca/.DS_Store\n",
      "Error processing: ./esca_dataset/validation/healthy/.DS_Store\n",
      "Removed: ./esca_dataset/validation/healthy/.DS_Store\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Supported image file formats\n",
    "supported_formats = ('JPEG', 'PNG', 'GIF', 'BMP')\n",
    "\n",
    "def filter_image_files(folder):\n",
    "    for dirpath, _, filenames in os.walk(folder):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            try:\n",
    "                # Attempt to open the image file\n",
    "                with Image.open(file_path) as img:\n",
    "                    # Check if the image format is supported\n",
    "                    if img.format not in supported_formats:\n",
    "                        # Remove the file if the format is not supported\n",
    "                        os.remove(file_path)\n",
    "                        print(f\"Removed: {file_path}\")\n",
    "            except (IOError, OSError):\n",
    "                # Handle errors (e.g., unable to open file)\n",
    "                print(f\"Error processing: {file_path}\")\n",
    "                os.remove(file_path)\n",
    "                print(f\"Removed: {file_path}\")\n",
    "\n",
    "# Filter image files in the folder and its subdirectories\n",
    "filter_image_files(data_folders['esca_dataset_swapped_bg']['.'])\n",
    "filter_image_files(data_folders['esca_dataset']['.'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T12:40:11.590761Z",
     "start_time": "2024-02-19T12:33:23.789642Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders combined successfully!\n"
     ]
    }
   ],
   "source": [
    "def combine_folders(source_folder1, source_folder2, destination_folder):\n",
    "    # Iterate over the subfolders of the source folders\n",
    "    for root, dirs, files in os.walk(source_folder1):\n",
    "        for file in files:\n",
    "            # Get the relative path of the file\n",
    "            relative_path = os.path.relpath(root, source_folder1)\n",
    "            # Create the corresponding destination directory\n",
    "            dest_dir = os.path.join(destination_folder, relative_path)\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "            # Copy the file from the first source folder to the destination folder\n",
    "            shutil.copy2(os.path.join(root, file), dest_dir)\n",
    "\n",
    "    # Iterate over the subfolders of the second source folder\n",
    "    for root, dirs, files in os.walk(source_folder2):\n",
    "        for file in files:\n",
    "            # Get the relative path of the file\n",
    "            relative_path = os.path.relpath(root, source_folder2)\n",
    "            # Create the corresponding destination directory\n",
    "            dest_dir = os.path.join(destination_folder, relative_path)\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "            # Copy the file from the second source folder to the destination folder\n",
    "            shutil.copy2(os.path.join(root, file), dest_dir)\n",
    "\n",
    "# Define the source folders and the destination folder\n",
    "esca_dataset = \"esca_dataset\"\n",
    "esca_dataset_swapped_bg = \"esca_dataset_swapped_bg\"\n",
    "esca_dataset_mixed = \"esca_dataset_mixed\"\n",
    "\n",
    "# Combine the folders\n",
    "combine_folders(esca_dataset, esca_dataset_swapped_bg, esca_dataset_mixed)\n",
    "\n",
    "print(\"Folders combined successfully!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T16:53:20.450156Z",
     "start_time": "2024-02-19T16:39:30.586931Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
